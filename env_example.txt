# ============================================
# ü§ñ YODA Configuration Template
# ============================================
# Copy this file to '.env' and fill in your values
# cp env_example.txt .env

# ============================================
# üîë API KEYS (REQUIRED)
# ============================================

# Datadog API Configuration
# Get these from: https://app.datadoghq.com/organization-settings/api-keys
DATADOG_API_KEY=your_datadog_api_key_here
DATADOG_APP_KEY=your_datadog_app_key_here

# Datadog Site (choose your region)
# Options: datadoghq.com, datadoghq.eu, us3.datadoghq.com, us5.datadoghq.com, ap1.datadoghq.com, ddog-gov.com
DATADOG_SITE=datadoghq.com

# OpenAI API Configuration
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# ============================================
# üåê LLM CONFIGURATION
# ============================================

# Custom LLM API URL (optional)
# Default: https://api.openai.com/v1/chat/completions
# For custom endpoints, local models, or other providers
# LLM_API_URL=http://localhost:1234/v1/chat/completions
# LLM_API_URL=https://api.anthropic.com/v1/messages
LLM_API_URL=https://api.openai.com/v1/chat/completions

# Conversation History Limit
# Number of recent conversations to keep in context (1-50)
# Default: 5 (5 user + 5 assistant = 10 messages total)
# Higher values = more context but more tokens used
# Lower values = less context but better performance
CONVERSATION_LIMIT=5

# Token Optimization Settings
# Maximum number of log entries to display (1-100)
# Default: 10 (prevents token overflow with large log results)
# Lower values = better token performance, fewer logs shown
LOG_DISPLAY_LIMIT=10

# Maximum characters per log message (20-500)
# Default: 80 (truncates long log messages to save tokens)
# Lower values = more aggressive truncation, better token performance
MAX_MESSAGE_LENGTH=80

# ============================================
# üîí SECURITY CONFIGURATION
# ============================================

# SSL Certificate Verification
# Default: true (recommended for production)
# Set to false only for development/testing with self-signed certificates
SSL_VERIFY=true

# ============================================
# üöÄ PERFORMANCE CONFIGURATION
# ============================================

# Cache Preloading at Startup
# Default: false (caches load on-demand)
# Set to true to pre-populate caches at application startup
# Improves first-use performance but increases startup time
PRELOAD_CACHES=false

# ============================================
# üí° USAGE EXAMPLES
# ============================================

# Development with local LLM:
# LLM_API_URL=http://localhost:1234/v1/chat/completions
# SSL_VERIFY=false
# CONVERSATION_LIMIT=3
# PRELOAD_CACHES=true

# Production settings:
# LLM_API_URL=https://api.openai.com/v1/chat/completions
# SSL_VERIFY=true
# CONVERSATION_LIMIT=5
# PRELOAD_CACHES=false

# High-context conversations:
# CONVERSATION_LIMIT=20

# Token-optimized (minimal context):
# CONVERSATION_LIMIT=2
# LOG_DISPLAY_LIMIT=5
# MAX_MESSAGE_LENGTH=50

# Ultra token-optimized (for 848k token errors):
# CONVERSATION_LIMIT=1
# LOG_DISPLAY_LIMIT=3
# MAX_MESSAGE_LENGTH=30 